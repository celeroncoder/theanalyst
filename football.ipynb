{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import make_scorer\n",
    "from time import time\n",
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_match_label(match):\n",
    "    ''' Derives a label for a given match. '''\n",
    "    \n",
    "    #Define variables\n",
    "    home_goals = match['home_team_goal']\n",
    "    away_goals = match['away_team_goal']\n",
    "     \n",
    "    label = pd.DataFrame()\n",
    "    label.loc[0,'match_api_id'] = match['match_api_id'] \n",
    "\n",
    "    #Identify match label  \n",
    "    if home_goals > away_goals:\n",
    "        label.loc[0,'label'] = \"Win\"\n",
    "    if home_goals == away_goals:\n",
    "        label.loc[0,'label'] = \"Draw\"\n",
    "    if home_goals < away_goals:\n",
    "        label.loc[0,'label'] = \"Defeat\"\n",
    "\n",
    "    #Return label        \n",
    "    return label.loc[0]\n",
    "    \n",
    "def get_fifa_stats(match, player_stats):\n",
    "    ''' Aggregates fifa stats for a given match. '''    \n",
    "    \n",
    "    #Define variables\n",
    "    match_id =  match.match_api_id\n",
    "    date = match['date']\n",
    "    players = ['home_player_1', 'home_player_2', 'home_player_3', \"home_player_4\", \"home_player_5\",\n",
    "               \"home_player_6\", \"home_player_7\", \"home_player_8\", \"home_player_9\", \"home_player_10\",\n",
    "               \"home_player_11\", \"away_player_1\", \"away_player_2\", \"away_player_3\", \"away_player_4\",\n",
    "               \"away_player_5\", \"away_player_6\", \"away_player_7\", \"away_player_8\", \"away_player_9\",\n",
    "               \"away_player_10\", \"away_player_11\"]\n",
    "    player_stats_new = pd.DataFrame()\n",
    "    names = []\n",
    "    \n",
    "    #Loop through all players\n",
    "    for player in players:   \n",
    "            \n",
    "        #Get player ID\n",
    "        player_id = match[player]\n",
    "        \n",
    "        #Get player stats \n",
    "        stats = player_stats[player_stats.player_api_id == player_id]\n",
    "            \n",
    "        #Identify current stats       \n",
    "        current_stats = stats[stats.date < date].sort_values(by = 'date', ascending = False)[:1]\n",
    "        \n",
    "        if np.isnan(player_id) == True:\n",
    "            overall_rating = pd.Series(0)\n",
    "        else:\n",
    "            current_stats.reset_index(inplace = True, drop = True)\n",
    "            overall_rating = pd.Series(current_stats.loc[0, \"overall_rating\"])\n",
    "\n",
    "        #Rename stat\n",
    "        name = \"{}_overall_rating\".format(player)\n",
    "        names.append(name)\n",
    "            \n",
    "        #Aggregate stats\n",
    "        player_stats_new = pd.concat([player_stats_new, overall_rating], axis = 1)\n",
    "    \n",
    "    player_stats_new.columns = names        \n",
    "    player_stats_new['match_api_id'] = match_id\n",
    "\n",
    "\n",
    "    player_stats_new.reset_index(inplace = True, drop = True)\n",
    "    \n",
    "    #Return player stats    \n",
    "    return player_stats_new.ix[0]     \n",
    "      \n",
    "def get_fifa_data(matches, player_stats, path = None, data_exists = False):\n",
    "    ''' Gets fifa data for all matches. '''  \n",
    "    \n",
    "    #Check if fifa data already exists\n",
    "    if data_exists == True:\n",
    "        \n",
    "        fifa_data = pd.read_pickle(path)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        print(\"Collecting fifa data for each match...\")       \n",
    "        start = time()\n",
    "        \n",
    "        #Apply get_fifa_stats for each match\n",
    "        fifa_data = matches.apply(lambda x :get_fifa_stats(x, player_stats), axis = 1)\n",
    "        \n",
    "        end = time()    \n",
    "        print(\"Fifa data collected in {:.1f} minutes\".format((end - start)/60))\n",
    "    \n",
    "    #Return fifa_data\n",
    "    return fifa_data\n",
    "\n",
    "def get_overall_fifa_rankings(fifa, get_overall = False):\n",
    "    ''' Get overall fifa rankings from fifa data. '''\n",
    "      \n",
    "    temp_data = fifa\n",
    "    \n",
    "    #Check if only overall player stats are desired\n",
    "    if get_overall == True:\n",
    "        \n",
    "        #Get overall stats\n",
    "        data = temp_data.loc[:,(fifa.columns.str.contains('overall_rating'))]\n",
    "        data.loc[:,'match_api_id'] = temp_data.loc[:,'match_api_id']\n",
    "    else:\n",
    "        \n",
    "        #Get all stats except for stat date\n",
    "        cols = fifa.loc[:,(fifa.columns.str.contains('date_stat'))]\n",
    "        temp_data = fifa.drop(cols.columns, axis = 1)        \n",
    "        data = temp_data\n",
    "    \n",
    "    #Return data\n",
    "    return data\n",
    "\n",
    "def get_last_matches(matches, date, team, x = 10):\n",
    "    ''' Get the last x matches of a given team. '''\n",
    "    \n",
    "    #Filter team matches from matches\n",
    "    team_matches = matches[(matches['home_team_api_id'] == team) | (matches['away_team_api_id'] == team)]\n",
    "                           \n",
    "    #Filter x last matches from team matches\n",
    "    last_matches = team_matches[team_matches.date < date].sort_values(by = 'date', ascending = False).iloc[0:x,:]\n",
    "    \n",
    "    #Return last matches\n",
    "    return last_matches\n",
    "    \n",
    "def get_last_matches_against_eachother(matches, date, home_team, away_team, x = 10):\n",
    "    ''' Get the last x matches of two given teams. '''\n",
    "    \n",
    "    #Find matches of both teams\n",
    "    home_matches = matches[(matches['home_team_api_id'] == home_team) & (matches['away_team_api_id'] == away_team)]    \n",
    "    away_matches = matches[(matches['home_team_api_id'] == away_team) & (matches['away_team_api_id'] == home_team)]  \n",
    "    total_matches = pd.concat([home_matches, away_matches])\n",
    "    \n",
    "    #Get last x matches\n",
    "    try:    \n",
    "        last_matches = total_matches[total_matches.date < date].sort_values(by = 'date', ascending = False).iloc[0:x,:]\n",
    "    except:\n",
    "        last_matches = total_matches[total_matches.date < date].sort_values(by = 'date', ascending = False).iloc[0:total_matches.shape[0],:]\n",
    "        \n",
    "        #Check for error in data\n",
    "        if(last_matches.shape[0] > x):\n",
    "            print(\"Error in obtaining matches\")\n",
    "            \n",
    "    #Return data\n",
    "    return last_matches\n",
    "    \n",
    "def get_goals(matches, team):\n",
    "    ''' Get the goals of a specfic team from a set of matches. '''\n",
    "    \n",
    "    #Find home and away goals\n",
    "    home_goals = int(matches.home_team_goal[matches.home_team_api_id == team].sum())\n",
    "    away_goals = int(matches.away_team_goal[matches.away_team_api_id == team].sum())\n",
    "\n",
    "    total_goals = home_goals + away_goals\n",
    "    \n",
    "    #Return total goals\n",
    "    return total_goals\n",
    "\n",
    "def get_goals_conceided(matches, team):\n",
    "    ''' Get the goals conceided of a specfic team from a set of matches. '''\n",
    "\n",
    "    #Find home and away goals\n",
    "    home_goals = int(matches.home_team_goal[matches.away_team_api_id == team].sum())\n",
    "    away_goals = int(matches.away_team_goal[matches.home_team_api_id == team].sum())\n",
    "\n",
    "    total_goals = home_goals + away_goals\n",
    "\n",
    "    #Return total goals\n",
    "    return total_goals\n",
    "\n",
    "def get_wins(matches, team):\n",
    "    ''' Get the number of wins of a specfic team from a set of matches. '''\n",
    "    \n",
    "    #Find home and away wins\n",
    "    home_wins = int(matches.home_team_goal[(matches.home_team_api_id == team) & (matches.home_team_goal > matches.away_team_goal)].count())\n",
    "    away_wins = int(matches.away_team_goal[(matches.away_team_api_id == team) & (matches.away_team_goal > matches.home_team_goal)].count())\n",
    "\n",
    "    total_wins = home_wins + away_wins\n",
    "\n",
    "    #Return total wins\n",
    "    return total_wins      \n",
    "    \n",
    "def get_match_features(match, matches, x = 10):\n",
    "    ''' Create match specific features for a given match. '''\n",
    "    \n",
    "    #Define variables\n",
    "    date = match.date\n",
    "    home_team = match.home_team_api_id\n",
    "    away_team = match.away_team_api_id\n",
    "    \n",
    "    #Get last x matches of home and away team\n",
    "    matches_home_team = get_last_matches(matches, date, home_team, x = 10)\n",
    "    matches_away_team = get_last_matches(matches, date, away_team, x = 10)\n",
    "    \n",
    "    #Get last x matches of both teams against each other\n",
    "    last_matches_against = get_last_matches_against_eachother(matches, date, home_team, away_team, x = 3)\n",
    "    \n",
    "    #Create goal variables\n",
    "    home_goals = get_goals(matches_home_team, home_team)\n",
    "    away_goals = get_goals(matches_away_team, away_team)\n",
    "    home_goals_conceided = get_goals_conceided(matches_home_team, home_team)\n",
    "    away_goals_conceided = get_goals_conceided(matches_away_team, away_team)\n",
    "    \n",
    "    #Define result data frame\n",
    "    result = pd.DataFrame()\n",
    "    \n",
    "    #Define ID features\n",
    "    result.loc[0, 'match_api_id'] = match.match_api_id\n",
    "    result.loc[0, 'league_id'] = match.league_id\n",
    "\n",
    "    #Create match features\n",
    "    result.loc[0, 'home_team_goals_difference'] = home_goals - home_goals_conceided\n",
    "    result.loc[0, 'away_team_goals_difference'] = away_goals - away_goals_conceided\n",
    "    result.loc[0, 'games_won_home_team'] = get_wins(matches_home_team, home_team) \n",
    "    result.loc[0, 'games_won_away_team'] = get_wins(matches_away_team, away_team)\n",
    "    result.loc[0, 'games_against_won'] = get_wins(last_matches_against, home_team)\n",
    "    result.loc[0, 'games_against_lost'] = get_wins(last_matches_against, away_team)\n",
    "    \n",
    "    #Return match features\n",
    "    return result.loc[0]\n",
    "    \n",
    "def create_feables(matches, fifa, bookkeepers, get_overall = False, horizontal = True, x = 10, verbose = True):\n",
    "    ''' Create and aggregate features and labels for all matches. '''\n",
    "\n",
    "    #Get fifa stats features\n",
    "    fifa_stats = get_overall_fifa_rankings(fifa, get_overall)\n",
    "    \n",
    "    \n",
    "    if verbose == True:\n",
    "        print(\"Generating match features...\")\n",
    "    start = time()\n",
    "    \n",
    "    #Get match features for all matches\n",
    "    match_stats = matches.apply(lambda x: get_match_features(x, matches, x = 10), axis = 1)\n",
    "    \n",
    "    #Create dummies for league ID feature\n",
    "    dummies = pd.get_dummies(match_stats['league_id']).rename(columns = lambda x: 'League_' + str(x))\n",
    "    match_stats = pd.concat([match_stats, dummies], axis = 1)\n",
    "    match_stats.drop(['league_id'], inplace = True, axis = 1)\n",
    "    \n",
    "    end = time()\n",
    "    if verbose == True:\n",
    "        print(\"Match features generated in {:.1f} minutes\".format((end - start)/60))\n",
    "    \n",
    "    if verbose == True:    \n",
    "        print(\"Generating match labels...\")\n",
    "    start = time()\n",
    "    \n",
    "    #Create match labels\n",
    "    labels = matches.apply(get_match_label, axis = 1)\n",
    "    end = time()\n",
    "    if verbose == True:\n",
    "        print(\"Match labels generated in {:.1f} minutes\".format((end - start)/60))\n",
    "    \n",
    "    if verbose == True:    \n",
    "        print(\"Generating bookkeeper data...\")\n",
    "    start = time()\n",
    "    \n",
    "    #Get bookkeeper quotas for all matches\n",
    "    bk_data = get_bookkeeper_data(matches, bookkeepers, horizontal = True)\n",
    "    bk_data.loc[:,'match_api_id'] = matches.loc[:,'match_api_id']\n",
    "    end = time()\n",
    "    if verbose == True:\n",
    "        print(\"Bookkeeper data generated in {:.1f} minutes\".format((end - start)/60))\n",
    "\n",
    "    #Merges features and labels into one frame\n",
    "    features = pd.merge(match_stats, fifa_stats, on = 'match_api_id', how = 'left')\n",
    "    features = pd.merge(features, bk_data, on = 'match_api_id', how = 'left')\n",
    "    feables = pd.merge(features, labels, on = 'match_api_id', how = 'left')\n",
    "    \n",
    "    #Drop NA values\n",
    "    feables.dropna(inplace = True)\n",
    "    \n",
    "    #Return preprocessed data\n",
    "    return feables\n",
    "    \n",
    "def train_classifier(clf, dm_reduction, X_train, y_train, cv_sets, params, scorer, jobs, use_grid_search = True, \n",
    "                     best_components = None, best_params = None):\n",
    "    ''' Fits a classifier to the training data. '''\n",
    "    \n",
    "    #Start the clock, train the classifier, then stop the clock\n",
    "    start = time()\n",
    "    \n",
    "    #Check if grid search should be applied\n",
    "    if use_grid_search == True: \n",
    "        \n",
    "        #Define pipeline of dm reduction and classifier\n",
    "        estimators = [('dm_reduce', dm_reduction), ('clf', clf)]\n",
    "        pipeline = Pipeline(estimators)\n",
    "        \n",
    "        #Grid search over pipeline and return best classifier\n",
    "        grid_obj = model_selection.GridSearchCV(pipeline, param_grid = params, scoring = scorer, cv = cv_sets, n_jobs = jobs)\n",
    "        grid_obj.fit(X_train, y_train)\n",
    "        best_pipe = grid_obj.best_estimator_\n",
    "    else:\n",
    "        \n",
    "        #Use best components that are known without grid search        \n",
    "        estimators = [('dm_reduce', dm_reduction(n_components = best_components)), ('clf', clf(best_params))]\n",
    "        pipeline = Pipeline(estimators)        \n",
    "        best_pipe = pipeline.fit(X_train, y_train)\n",
    "        \n",
    "    end = time()\n",
    "    \n",
    "    #Print the results\n",
    "    print(\"Trained {} in {:.1f} minutes\".format(clf.__class__.__name__, (end - start)/60))\n",
    "    \n",
    "    #Return best pipe\n",
    "    return best_pipe\n",
    "    \n",
    "def predict_labels(clf, best_pipe, features, target):\n",
    "    ''' Makes predictions using a fit classifier based on scorer. '''\n",
    "    \n",
    "    #Start the clock, make predictions, then stop the clock\n",
    "    start = time()\n",
    "    y_pred = clf.predict(best_pipe.named_steps['dm_reduce'].transform(features))\n",
    "    end = time()\n",
    "    \n",
    "    #Print and return results\n",
    "    print(\"Made predictions in {:.4f} seconds\".format(end - start))\n",
    "    return accuracy_score(target.values, y_pred)\n",
    "    \n",
    "def train_calibrate_predict(clf, dm_reduction, X_train, y_train, X_calibrate, y_calibrate, X_test, y_test, cv_sets, params, scorer, jobs, \n",
    "                            use_grid_search = True, **kwargs):\n",
    "    ''' Train and predict using a classifer based on scorer. '''\n",
    "    \n",
    "    #Indicate the classifier and the training set size\n",
    "    print(\"Training a {} with {}...\".format(clf.__class__.__name__, dm_reduction.__class__.__name__))\n",
    "    \n",
    "    #Train the classifier\n",
    "    best_pipe = train_classifier(clf, dm_reduction, X_train, y_train, cv_sets, params, scorer, jobs)\n",
    "    \n",
    "    #Calibrate classifier\n",
    "    print(\"Calibrating probabilities of classifier...\")\n",
    "    start = time()    \n",
    "    clf = CalibratedClassifierCV(best_pipe.named_steps['clf'], cv= 'prefit', method='isotonic')\n",
    "    clf.fit(best_pipe.named_steps['dm_reduce'].transform(X_calibrate), y_calibrate)\n",
    "    end = time()\n",
    "    print(\"Calibrated {} in {:.1f} minutes\".format(clf.__class__.__name__, (end - start)/60))\n",
    "    \n",
    "    # Print the results of prediction for both training and testing\n",
    "    print(\"Score of {} for training set: {:.4f}.\".format(clf.__class__.__name__, predict_labels(clf, best_pipe, X_train, y_train)))\n",
    "    print(\"Score of {} for test set: {:.4f}.\".format(clf.__class__.__name__, predict_labels(clf, best_pipe, X_test, y_test)))\n",
    "    \n",
    "    #Return classifier, dm reduction, and label predictions for train and test set\n",
    "    return clf, best_pipe.named_steps['dm_reduce'], predict_labels(clf, best_pipe, X_train, y_train), predict_labels(clf, best_pipe, X_test, y_test)\n",
    "        \n",
    "def convert_odds_to_prob(match_odds):\n",
    "    ''' Converts bookkeeper odds to probabilities. '''\n",
    "    \n",
    "    #Define variables\n",
    "    match_id = match_odds.loc[:,'match_api_id']\n",
    "    bookkeeper = match_odds.loc[:,'bookkeeper']    \n",
    "    win_odd = match_odds.loc[:,'Win']\n",
    "    draw_odd = match_odds.loc[:,'Draw']\n",
    "    loss_odd = match_odds.loc[:,'Defeat']\n",
    "    \n",
    "    #Converts odds to prob\n",
    "    win_prob = 1 / win_odd\n",
    "    draw_prob = 1 / draw_odd\n",
    "    loss_prob = 1 / loss_odd\n",
    "    \n",
    "    total_prob = win_prob + draw_prob + loss_prob\n",
    "    \n",
    "    probs = pd.DataFrame()\n",
    "    \n",
    "    #Define output format and scale probs by sum over all probs\n",
    "    probs.loc[:,'match_api_id'] = match_id\n",
    "    probs.loc[:,'bookkeeper'] = bookkeeper\n",
    "    probs.loc[:,'Win'] = win_prob / total_prob\n",
    "    probs.loc[:,'Draw'] = draw_prob / total_prob\n",
    "    probs.loc[:,'Defeat'] = loss_prob / total_prob\n",
    "    \n",
    "    #Return probs and meta data\n",
    "    return probs\n",
    "    \n",
    "def get_bookkeeper_data(matches, bookkeepers, horizontal = True):\n",
    "    ''' Aggregates bookkeeper data for all matches and bookkeepers. '''\n",
    "    \n",
    "    bk_data = pd.DataFrame()\n",
    "    \n",
    "    #Loop through bookkeepers\n",
    "    for bookkeeper in bookkeepers:\n",
    "\n",
    "        #Find columns containing data of bookkeeper\n",
    "        temp_data = matches.loc[:,(matches.columns.str.contains(bookkeeper))]\n",
    "        temp_data.loc[:, 'bookkeeper'] = str(bookkeeper)\n",
    "        temp_data.loc[:, 'match_api_id'] = matches.loc[:, 'match_api_id']\n",
    "        \n",
    "        #Rename odds columns and convert to numeric\n",
    "        cols = temp_data.columns.values\n",
    "        cols[:3] = ['Win','Draw','Defeat']\n",
    "        temp_data.columns = cols\n",
    "        temp_data.loc[:,'Win'] = pd.to_numeric(temp_data['Win'])\n",
    "        temp_data.loc[:,'Draw'] = pd.to_numeric(temp_data['Draw'])\n",
    "        temp_data.loc[:,'Defeat'] = pd.to_numeric(temp_data['Defeat'])\n",
    "        \n",
    "        #Check if data should be aggregated horizontally\n",
    "        if(horizontal == True):\n",
    "            \n",
    "            #Convert data to probs\n",
    "            temp_data = convert_odds_to_prob(temp_data)\n",
    "            temp_data.drop('match_api_id', axis = 1, inplace = True)\n",
    "            temp_data.drop('bookkeeper', axis = 1, inplace = True)\n",
    "            \n",
    "            #Rename columns with bookkeeper names\n",
    "            win_name = bookkeeper + \"_\" + \"Win\"\n",
    "            draw_name = bookkeeper + \"_\" + \"Draw\"\n",
    "            defeat_name = bookkeeper + \"_\" + \"Defeat\"\n",
    "            temp_data.columns.values[:3] = [win_name, draw_name, defeat_name]\n",
    "\n",
    "            #Aggregate data\n",
    "            bk_data = pd.concat([bk_data, temp_data], axis = 1)\n",
    "        else:\n",
    "            #Aggregate vertically\n",
    "            bk_data = bk_data.append(temp_data, ignore_index = True)\n",
    "    \n",
    "    #If horizontal add match api id to data\n",
    "    if(horizontal == True):\n",
    "        temp_data.loc[:, 'match_api_id'] = matches.loc[:, 'match_api_id']\n",
    "    \n",
    "    #Return bookkeeper data\n",
    "    return bk_data\n",
    "    \n",
    "def get_bookkeeper_probs(matches, bookkeepers, horizontal = False):\n",
    "    ''' Get bookkeeper data and convert to probabilities for vertical aggregation. '''\n",
    "    \n",
    "    #Get bookkeeper data\n",
    "    data = get_bookkeeper_data(matches, bookkeepers, horizontal = False)\n",
    "    \n",
    "    #Convert odds to probabilities\n",
    "    probs = convert_odds_to_prob(data)\n",
    "    \n",
    "    #Return data\n",
    "    return probs\n",
    "\n",
    "def plot_confusion_matrix(y_test, X_test, clf, dim_reduce, path, cmap=plt.cm.Blues, normalize = False):    \n",
    "    ''' Plot confusion matrix for given classifier and data. '''\n",
    "    \n",
    "    #Define label names and get confusion matrix values\n",
    "    labels = [\"Win\", \"Draw\", \"Defeat\"]\n",
    "    cm = confusion_matrix(y_test, clf.predict(dim_reduce.transform(X_test)), labels)\n",
    "    \n",
    "    #Check if matrix should be normalized\n",
    "    if normalize == True:\n",
    "        \n",
    "        #Normalize\n",
    "        cm = cm.astype('float') / cm.sum()\n",
    "        \n",
    "    #Configure figure\n",
    "    sns.set_style(\"whitegrid\", {\"axes.grid\" : False})\n",
    "    fig = plt.figure(1)    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap = plt.cm.Blues)\n",
    "    title= \"Confusion matrix of a {} with {}\".format(best_clf.base_estimator.__class__.__name__, best_dm_reduce.__class__.__name__)   \n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, round(cm[i, j], 2),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    #Print classification report\n",
    "    y_pred = clf.predict(dim_reduce.transform(X_test))\n",
    "    print(classification_report(y_test, y_pred)) \n",
    "\n",
    "def compare_probabilities(clf, dim_reduce, bk, bookkeepers, matches, fifa_data, verbose = False):\n",
    "    ''' Map bookkeeper and model probabilities. '''\n",
    "    \n",
    "    #Create features and labels for given matches\n",
    "    feables = create_feables(matches, fifa_data, bk, get_overall = True, verbose = False)\n",
    "    \n",
    "    #Ensure consistency\n",
    "    match_ids = list(feables['match_api_id'])\n",
    "    matches = matches[matches['match_api_id'].isin(match_ids)]\n",
    "    \n",
    "    #Get bookkeeper probabilities\n",
    "    if verbose == True:\n",
    "        print(\"Obtaining bookkeeper probabilities...\")\n",
    "    bookkeeper_probs = get_bookkeeper_probs(matches, bookkeepers)\n",
    "    bookkeeper_probs.reset_index(inplace = True, drop = True)\n",
    "    \n",
    "    inputs = feables.drop('match_api_id', axis = 1)\n",
    "    labels = inputs.loc[:,'label']\n",
    "    features = inputs.drop('label', axis = 1)\n",
    "    \n",
    "    #Get model probabilities\n",
    "    if verbose == True:\n",
    "        print(\"Predicting probabilities based on model...\")\n",
    "    model_probs = pd.DataFrame()\n",
    "    label_table = pd.Series()\n",
    "    temp_probs = pd.DataFrame(clf.predict_proba(dim_reduce.transform(features)), columns = ['win_prob', 'draw_prob', 'defeat_prob'])\n",
    "    for bookkeeper in bookkeepers:\n",
    "        model_probs = model_probs.append(temp_probs, ignore_index = True)\n",
    "        label_table = label_table.append(labels)\n",
    "    model_probs.reset_index(inplace = True, drop = True)\n",
    "    label_table.reset_index(inplace = True, drop = True)\n",
    "    bookkeeper_probs['win_prob'] = model_probs['win_prob']\n",
    "    bookkeeper_probs['draw_prob'] = model_probs['draw_prob']\n",
    "    bookkeeper_probs['defeat_prob'] = model_probs['defeat_prob']\n",
    "    bookkeeper_probs['label'] = label_table \n",
    "    \n",
    "    #Aggregate win probabilities for each match\n",
    "    wins = bookkeeper_probs[['bookkeeper', 'match_api_id', 'Win', 'win_prob', 'label']]\n",
    "    wins.loc[:, 'bet'] = 'Win'\n",
    "    wins = wins.rename(columns = {'Win':'bookkeeper_prob',\n",
    "                                  'win_prob': 'model_prob'})\n",
    "                                  \n",
    "    #Aggregate draw probabilities for each match\n",
    "    draws = bookkeeper_probs[['bookkeeper', 'match_api_id', 'Draw', 'draw_prob', 'label']]\n",
    "    draws.loc[:, 'bet'] = 'Draw'\n",
    "    draws = draws.rename(columns = {'Draw':'bookkeeper_prob',\n",
    "                                  'draw_prob': 'model_prob'})\n",
    "                                  \n",
    "    #Aggregate defeat probabilities for each match\n",
    "    defeats = bookkeeper_probs[['bookkeeper', 'match_api_id', 'Defeat', 'defeat_prob', 'label']]\n",
    "    defeats.loc[:, 'bet'] = 'Defeat'\n",
    "    defeats = defeats.rename(columns = {'Defeat':'bookkeeper_prob',\n",
    "                                  'defeat_prob': 'model_prob'})\n",
    "    \n",
    "    total = pd.concat([wins, draws, defeats])\n",
    "    \n",
    "    #Return total\n",
    "    return total\n",
    "    \n",
    "def find_good_bets(clf, dim_reduce, bk, bookkeepers, matches, fifa_data, percentile, prob_cap, verbose = False):\n",
    "    ''' Find good bets for a given classifier and matches. '''\n",
    "    \n",
    "    #Compare model and classifier probabilities\n",
    "    probs = compare_probabilities(clf, dim_reduce, bk, bookkeepers, matches, fifa_data, verbose = False)\n",
    "    probs.loc[:, 'prob_difference'] = probs.loc[:,\"model_prob\"] - probs.loc[:,\"bookkeeper_prob\"]\n",
    "    \n",
    "    #Sort by createst difference to identify most underestimated bets    \n",
    "    values = probs['prob_difference']\n",
    "    values = values.sort_values(ascending = False)\n",
    "    values.reset_index(inplace = True, drop = True)\n",
    "    \n",
    "    if verbose == True:\n",
    "        print(\"Selecting attractive bets...\")\n",
    "        \n",
    "    #Identify choices that fulfill requirements such as positive difference, minimum probability and match outcome\n",
    "    relevant_choices = probs[(probs.prob_difference > 0) & (probs.model_prob > prob_cap) & (probs.bet != \"Draw\")]\n",
    "    \n",
    "    #Select given percentile of relevant choices    \n",
    "    top_percent = 1 - percentile\n",
    "    choices = relevant_choices[relevant_choices.prob_difference >= relevant_choices.prob_difference.quantile(top_percent)]\n",
    "    choices.reset_index(inplace = True, drop = True)\n",
    "    \n",
    "    #Return choices\n",
    "    return choices\n",
    "\n",
    "def get_reward(choice, matches):\n",
    "    ''' Get the reward of a given bet. '''\n",
    "    \n",
    "    #Identify bet\n",
    "    match = matches[matches.match_api_id == choice.match_api_id]\n",
    "    bet_data = match.loc[:,(match.columns.str.contains(choice.bookkeeper))]\n",
    "    cols = bet_data.columns.values\n",
    "    cols[:3] = ['win','draw','defeat']\n",
    "    bet_data.columns = cols\n",
    "    \n",
    "    #Identfiy bet type and get quota\n",
    "    if choice.bet == 'Win':\n",
    "        bet_quota = bet_data.win.values\n",
    "    elif choice.bet == 'Draw':\n",
    "        bet_quota = bet_data.draw.values\n",
    "    elif choice.bet == 'Defeat':\n",
    "        bet_quota = bet_data.defeat.values\n",
    "    else:\n",
    "        print(\"Error\")\n",
    "    \n",
    "    #Check label and compute reward\n",
    "    if choice.bet == choice.label:\n",
    "        reward = bet_quota\n",
    "    else:\n",
    "        reward = 0\n",
    "    \n",
    "    #Return reward\n",
    "    return reward\n",
    "      \n",
    "def execute_bets(bet_choices, matches, verbose = False):\n",
    "    ''' Get rewards for all bets. '''    \n",
    "    \n",
    "    if verbose == True:\n",
    "        print(\"Obtaining reward for chosen bets...\")\n",
    "    total_reward = 0\n",
    "    total_invested = 0\n",
    "    \n",
    "    #Loop through bets\n",
    "    loops = np.arange(0, bet_choices.shape[0])     \n",
    "    for i in loops:\n",
    "        \n",
    "        #Get rewards and accumulate profit\n",
    "        reward = get_reward(bet_choices.iloc[i,:], matches)\n",
    "        total_reward = total_reward + reward\n",
    "        total_invested += 1\n",
    "    \n",
    "    #Compute investment return\n",
    "    investment_return = float(total_reward / total_invested) - 1\n",
    "    \n",
    "    #Return investment return\n",
    "    return investment_return\n",
    "    \n",
    "def explore_data(features, inputs, path):\n",
    "    ''' Explore data by plotting KDE graphs. '''\n",
    "    \n",
    "    #Define figure subplots\n",
    "    fig = plt.figure(1)\n",
    "    fig.subplots_adjust(bottom= -1, left=0.025, top = 2, right=0.975)\n",
    "    \n",
    "    #Loop through features    \n",
    "    i = 1\n",
    "    for col in features.columns:\n",
    "        \n",
    "        #Set subplot and plot format        \n",
    "        sns.set_style(\"whitegrid\")\n",
    "        sns.set_context(\"paper\", font_scale = 0.5, rc={\"lines.linewidth\": 1})\n",
    "        plt.subplot(7,7,0 + i)\n",
    "        j = i - 1\n",
    "        \n",
    "        #Plot KDE for all labels\n",
    "        sns.distplot(inputs[inputs['label'] == 'Win'].iloc[:,j], hist = False, label = 'Win')\n",
    "        sns.distplot(inputs[inputs['label'] == 'Draw'].iloc[:,j], hist = False, label = 'Draw')\n",
    "        sns.distplot(inputs[inputs['label'] == 'Defeat'].iloc[:,j], hist = False, label = 'Defeat')\n",
    "        plt.legend();\n",
    "        i = i + 1\n",
    "    \n",
    "    #Define plot format    \n",
    "    DefaultSize = fig.get_size_inches()\n",
    "    fig.set_size_inches((DefaultSize[0]*1.2, DefaultSize[1]*1.2))\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    #Compute and print label weights\n",
    "    labels = inputs.loc[:,'label']\n",
    "    class_weights = labels.value_counts() / len(labels)\n",
    "    print(class_weights)\n",
    "    \n",
    "    #Store description of all features\n",
    "    feature_details = features.describe().transpose()\n",
    "\n",
    "    #Return feature details\n",
    "    return feature_details\n",
    "    \n",
    "def find_best_classifier(classifiers, dm_reductions, scorer, X_t, y_t, X_c, y_c, X_v, y_v, cv_sets, params, jobs):\n",
    "    ''' Tune all classifier and dimensionality reduction combiantions to find best classifier. '''\n",
    "    \n",
    "    #Initialize result storage\n",
    "    clfs_return = []\n",
    "    dm_reduce_return = []\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    \n",
    "    #Loop through dimensionality reductions\n",
    "    for dm in dm_reductions:\n",
    "        \n",
    "        #Loop through classifiers\n",
    "        for clf in clfs:\n",
    "            \n",
    "            #Grid search, calibrate, and test the classifier\n",
    "            clf, dm_reduce, train_score, test_score = train_calibrate_predict(clf = clf, dm_reduction = dm, X_train = X_t, y_train = y_t,\n",
    "                                                      X_calibrate = X_c, y_calibrate = y_c,\n",
    "                                                      X_test = X_v, y_test = y_v, cv_sets = cv_sets,\n",
    "                                                      params = params[clf], scorer = scorer, jobs = jobs, use_grid_search = True)\n",
    "            \n",
    "            #Append the result to storage            \n",
    "            clfs_return.append(clf)\n",
    "            dm_reduce_return.append(dm_reduce)\n",
    "            train_scores.append(train_score)\n",
    "            test_scores.append(test_score)\n",
    "    \n",
    "    #Return storage\n",
    "    return clfs_return, dm_reduce_return, train_scores, test_scores\n",
    "\n",
    "def plot_training_results(clfs, dm_reductions, train_scores, test_scores, path):\n",
    "    ''' Plot results of classifier training. '''\n",
    "    \n",
    "    #Set graph format\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_context(\"paper\", font_scale = 1, rc={\"lines.linewidth\": 1})\n",
    "    ax = plt.subplot(111)\n",
    "    w = 0.5\n",
    "    x = np.arange(len(train_scores))\n",
    "    ax.set_yticks(x + w)\n",
    "    ax.legend((train_scores[0], test_scores[0]), (\"Train Scores\", \"Test Scores\"))\n",
    "    names = []\n",
    "    \n",
    "    #Loop throuugh classifiers\n",
    "    for i in range(0, len(clfs)): \n",
    "        \n",
    "        #Define temporary variables        \n",
    "        clf = clfs[i]\n",
    "        clf_name = clf.base_estimator.__class__.__name__\n",
    "        dm = dm_reductions[i]\n",
    "        dm_name = dm.__class__.__name__\n",
    "        \n",
    "        #Create and store name\n",
    "        name = \"{} with {}\".format(clf_name, dm_name)\n",
    "        names.append(name)\n",
    "        \n",
    "    #Plot all names in horizontal bar plot\n",
    "    ax.set_yticklabels((names))\n",
    "    plt.xlim(0.5, 0.55)\n",
    "    plt.barh(x, test_scores, color = 'b', alpha = 0.6)\n",
    "    plt.title(\"Test Data Accuracy Scores\")\n",
    "    fig = plt.figure(1)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "def optimize_betting(best_clf, best_dm_reduce, bk_cols_selected, bk_cols, match_data, fifa_data,\n",
    "                     n_samples, sample_size, parameter_1_grid, parameter_2_grid, verbose = False):\n",
    "    ''' Tune parameters of bet selection algorithm. '''\n",
    "    \n",
    "    #Generate data samples\n",
    "    samples = []\n",
    "    for i in range(0, n_samples):\n",
    "        sample = match_data.sample(n = sample_size, random_state = 42)\n",
    "        samples.append(sample)\n",
    "    \n",
    "    results = pd.DataFrame(columns = [\"parameter_1\", \"parameter_2\", \"results\"])\n",
    "    row = 0\n",
    "    \n",
    "    #Iterate over all 1 parameter\n",
    "    for i in parameter_1_grid:\n",
    "        \n",
    "        #Iterate over all 2 parameter\n",
    "        for j in parameter_2_grid:\n",
    "            \n",
    "            #Compute average score over all samples\n",
    "            profits = []\n",
    "            for sample in samples:\n",
    "                choices = find_good_bets(best_clf, best_dm_reduce, bk_cols_selected, bk_cols, sample, fifa_data, i, j)\n",
    "                profit = execute_bets(choices, match_data)\n",
    "                profits.append(profit)\n",
    "            result = np.mean(np.array(profits))\n",
    "            results.loc[row,\"results\"] = result\n",
    "            results.loc[row,\"parameter_1\"] = i\n",
    "            results.loc[row,\"parameter_2\"] = j\n",
    "            row = row + 1\n",
    "            if verbose == True: print(\"Simulated parameter combination: {}\".format(row))\n",
    "               \n",
    "    #Return best setting and result\n",
    "    best_result = results.ix[results['results'].idxmax()] \n",
    "    return best_result\n",
    "    \n",
    "    \n",
    "def plot_bookkeeper_cf_matrix(matches, bookkeepers, path, verbose = False, normalize = True):\n",
    "    ''' Plot confusion matrix of bookkeeper predictions. '''\n",
    "    \n",
    "    if verbose == True: print(\"Obtaining labels...\")\n",
    "    \n",
    "    #Get match labels\n",
    "    y_test_temp = matches.apply(get_match_label, axis = 1)\n",
    "    \n",
    "    if verbose == True: print(\"Obtaining bookkeeper probabilities...\")\n",
    "    \n",
    "    #Get bookkeeper probabilities\n",
    "    bookkeeper_probs = get_bookkeeper_probs(matches, bookkeepers)\n",
    "    bookkeeper_probs.reset_index(inplace = True, drop = True)\n",
    "    bookkeeper_probs.dropna(inplace = True)\n",
    "    \n",
    "    if verbose == True: print(\"Obtaining bookkeeper labels...\")\n",
    "    \n",
    "    #Get bookkeeper labels\n",
    "    y_pred_temp = pd.DataFrame()\n",
    "    y_pred_temp.loc[:,'bk_label'] = bookkeeper_probs[['Win', 'Draw', 'Defeat']].idxmax(axis = 1)\n",
    "    y_pred_temp.loc[:,'match_api_id'] = bookkeeper_probs.loc[:, 'match_api_id']\n",
    "    \n",
    "    if verbose == True: print(\"Plotting confusion matrix...\")\n",
    "    \n",
    "    #Format data\n",
    "    results = pd.merge(y_pred_temp, y_test_temp, on = 'match_api_id', how = 'left')\n",
    "    y_test = results.loc[:, 'label']\n",
    "    y_pred = results.loc[:, 'bk_label']\n",
    "    \n",
    "    #Generate confusion matrix\n",
    "    labels = [\"Win\", \"Draw\", \"Defeat\"]\n",
    "    cm = confusion_matrix(y_test, y_pred, labels) \n",
    "    \n",
    "    #Check for normalization\n",
    "    if normalize == True:\n",
    "        cm = cm.astype('float') / cm.sum()\n",
    "        \n",
    "    #Plot confusion matrix\n",
    "    sns.set_style(\"whitegrid\", {\"axes.grid\" : False})\n",
    "    fig = plt.figure(1)    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap = plt.cm.Blues)\n",
    "    title = \"Confusion matrix of Bookkeeper predictions!\"   \n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, round(cm[i, j], 2),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    #Print classification report and accuracy score of bookkeepers\n",
    "    print(classification_report(y_test, y_pred)) \n",
    "    print(\"Bookkeeper score for test set: {:.4f}.\".format(accuracy_score(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "## Fetching data\n",
    "#Connecting to database\n",
    "path = \"../input/\"  #Insert path here\n",
    "database = path + 'database.sqlite'\n",
    "conn = sqlite3.connect(database)\n",
    "\n",
    "#Defining the number of jobs to be run in parallel during grid search\n",
    "n_jobs = 1 #Insert number of parallel jobs here\n",
    "\n",
    "#Fetching required data tables\n",
    "player_data = pd.read_sql(\"SELECT * FROM Player;\", conn)\n",
    "player_stats_data = pd.read_sql(\"SELECT * FROM Player_Attributes;\", conn)\n",
    "team_data = pd.read_sql(\"SELECT * FROM Team;\", conn)\n",
    "match_data = pd.read_sql(\"SELECT * FROM Match;\", conn)\n",
    "\n",
    "#Reduce match data to fulfill run time requirements\n",
    "rows = [\"country_id\", \"league_id\", \"season\", \"stage\", \"date\", \"match_api_id\", \"home_team_api_id\", \n",
    "        \"away_team_api_id\", \"home_team_goal\", \"away_team_goal\", \"home_player_1\", \"home_player_2\",\n",
    "        \"home_player_3\", \"home_player_4\", \"home_player_5\", \"home_player_6\", \"home_player_7\", \n",
    "        \"home_player_8\", \"home_player_9\", \"home_player_10\", \"home_player_11\", \"away_player_1\",\n",
    "        \"away_player_2\", \"away_player_3\", \"away_player_4\", \"away_player_5\", \"away_player_6\",\n",
    "        \"away_player_7\", \"away_player_8\", \"away_player_9\", \"away_player_10\", \"away_player_11\"]\n",
    "match_data.dropna(subset = rows, inplace = True)\n",
    "match_data = match_data.tail(1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fifa_data = get_fifa_data(match_data, player_stats_data, data_exists = False)\n",
    "\n",
    "#Creating features and labels based on data provided\n",
    "bk_cols = ['B365', 'BW', 'IW', 'LB', 'PS', 'WH', 'SJ', 'VC', 'GB', 'BS']\n",
    "bk_cols_selected = ['B365', 'BW']      \n",
    "feables = create_feables(match_data, fifa_data, bk_cols_selected, get_overall = True)\n",
    "inputs = feables.drop('match_api_id', axis = 1)\n",
    "\n",
    "#Exploring the data and creating visualizations\n",
    "labels = inputs.loc[:,'label']\n",
    "features = inputs.drop('label', axis = 1)\n",
    "features.head(5)\n",
    "feature_details = explore_data(features, inputs, path)\n",
    "\n",
    "#Splitting the data into Train, Calibrate, and Test data sets\n",
    "X_train_calibrate, X_test, y_train_calibrate, y_test = train_test_split(features, labels, test_size = 0.2, random_state = 42, \n",
    "                                                                        stratify = labels)\n",
    "X_train, X_calibrate, y_train, y_calibrate = train_test_split(X_train_calibrate, y_train_calibrate, test_size = 0.3, random_state = 42, \n",
    "                                                              stratify = y_train_calibrate)\n",
    "\n",
    "#Creating cross validation data splits\n",
    "cv_sets = model_selection.StratifiedShuffleSplit(n_splits = 5, test_size = 0.20, random_state = 5)\n",
    "cv_sets.get_n_splits(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_clf = RandomForestClassifier(n_estimators = 200, random_state = 1, class_weight = 'balanced')\n",
    "AB_clf = AdaBoostClassifier(n_estimators = 200, random_state = 2)\n",
    "GNB_clf = GaussianNB()\n",
    "KNN_clf =  KNeighborsClassifier()\n",
    "LOG_clf = linear_model.LogisticRegression(multi_class = \"ovr\", solver = \"sag\", class_weight = 'balanced')\n",
    "clfs = [RF_clf, AB_clf, GNB_clf, KNN_clf, LOG_clf]\n",
    "\n",
    "#Specficying scorer and parameters for grid search\n",
    "feature_len = features.shape[1]\n",
    "scorer = make_scorer(accuracy_score)\n",
    "parameters_RF = {'clf__max_features': ['auto', 'log2'], \n",
    "                 'dm_reduce__n_components': np.arange(5, feature_len, np.around(feature_len/5))}\n",
    "parameters_AB = {'clf__learning_rate': np.linspace(0.5, 2, 5), \n",
    "                 'dm_reduce__n_components': np.arange(5, feature_len, np.around(feature_len/5))}\n",
    "parameters_GNB = {'dm_reduce__n_components': np.arange(5, feature_len, np.around(feature_len/5))}\n",
    "parameters_KNN = {'clf__n_neighbors': [3, 5, 10], \n",
    "                  'dm_reduce__n_components': np.arange(5, feature_len, np.around(feature_len/5))}\n",
    "parameters_LOG = {'clf__C': np.logspace(1, 1000, 5), \n",
    "                  'dm_reduce__n_components': np.arange(5, feature_len, np.around(feature_len/5))}\n",
    "\n",
    "parameters = {clfs[0]: parameters_RF,\n",
    "              clfs[1]: parameters_AB,\n",
    "              clfs[2]: parameters_GNB,\n",
    "              clfs[3]: parameters_KNN,\n",
    "              clfs[4]: parameters_LOG}\n",
    "\n",
    "#Initializing dimensionality reductions\n",
    "pca = PCA()\n",
    "dm_reductions = [pca]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LOG_clf\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Score of {} for training set: {:.4f}.\".format(clf.__class__.__name__, accuracy_score(y_train, clf.predict(X_train))))\n",
    "print(\"Score of {} for test set: {:.4f}.\".format(clf.__class__.__name__, accuracy_score(y_test, clf.predict(X_test))))\n",
    "\n",
    "#Training all classifiers and comparing them\n",
    "clfs, dm_reductions, train_scores, test_scores = find_best_classifier(clfs, dm_reductions, scorer, X_train, y_train, \n",
    "                                                                    X_calibrate, y_calibrate, X_test, y_test, cv_sets, \n",
    "                                                                      parameters, n_jobs)\n",
    "\n",
    "#Plotting train and test scores\n",
    "plot_training_results(clfs, dm_reductions, np.array(train_scores), np.array(test_scores), path = path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf = clfs[np.argmax(test_scores)]\n",
    "best_dm_reduce = dm_reductions[np.argmax(test_scores)]\n",
    "print(\"The best classifier is a {} with {}.\".format(best_clf.base_estimator.__class__.__name__, best_dm_reduce.__class__.__name__)                              )\n",
    "plot_confusion_matrix(y_test, X_test, best_clf, best_dm_reduce, path = path, normalize = True)\n",
    "\n",
    "#Plotting a confusion matrix of bookkeepers\n",
    "plot_bookkeeper_cf_matrix(match_data, bk_cols, path, verbose = True, normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_grid = np.linspace(0, 1, 2)\n",
    "probability_grid = np.linspace(0, 0.5, 2)\n",
    "best_betting = optimize_betting(best_clf, best_dm_reduce, bk_cols_selected, bk_cols, match_data, fifa_data,\n",
    "                     5, 300, percentile_grid, probability_grid, verbose = True)\n",
    "end = time()    \n",
    "print(\"Program run in {:.1f} minutes\".format((end - start)/60))\n",
    "print(\"The best return of investment is: {}\".format(best_betting.results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
